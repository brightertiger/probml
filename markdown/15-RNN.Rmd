# Recurrent NN

- RNN maps input sequences to output space in a stateful way
- Output y(t) not only depends on x(t) but also a hidden state h(t)
- Hidden state gets updated over time as the sequence is processed

\

- Vec2Seq (Sequence Generation)
    - Input is a vector
    - Output is a sequence of arbitrary length
    - Output sequence is generated one token at a time
        - $p(y_{1:T} | x) = \sum p(y_{1:T}, h_{1:T} | x)$
        - $p(y_{1:T} | x) = \sum \prod p(y_t | h_t) \times p(h_t | h_{t-1} , y_{t-1}, x)$
        - $p(y_t | h_t)$ can be:
            - Categorical 
            - Gaussian
        - $h_t = \phi( W_{xh}[x;y_{t-1}] + W_{hh}h_{t-1} + b_h)$
            - W(x,h) are input to hidden weights
            - W(h,h) are hidden to hidden weights
    - RNNs can have unbounded memory unlike Markov models

\

- Seq2Vec (Sequence Classification)
    - Input is a variable length sequence
    - Output is a fixed dimension vector
    - For example: Classification Task
        - $p(y|x_{1:T}) = \text{Cat}(y|S(WH_T))$
    - Results can be improved if model can depend on both past and future context
        - Apply bidirectional RNN
        - $h^{\rightarrow} = \phi(W_{xh}^{\rightarrow}x_t + W_{hh}^{\rightarrow}h_t)$
        - $h^{\leftarrow} = \phi(W_{xh}^{\leftarrow}x_t + W_{hh}^{\leftarrow}h_t)$
        - Input to the linear layer is concatenation of the two hidden states

\
  
- Seq2Seq (Sequence Translation)
    - Input is a variable length sequence
    - Output is a variable length sequence
    - Aligned Case:
        - If input and output length are the same
        - One label prediction per step
        - $p(y_{1:T}|h_{1:T}) = \sum \prod p(y_t | h_t) I\{h_t = f(h_{t-1},x_t)\}$
    - Unaligned Case
        - If input and output length are not the same
        - Encoder-Decoder architecture
        - Encode the sequence to get the context vector
        - Generate the output sequence using the decoder
        - Teacher Forcing
            - While training the model, ground truth is fed to the model and not the labels generated by the model
            - Teacher’s values are force fed to the model
            - Sometimes results in poor test time performance
            - Scheduled Sampling
                - Start with teacher forcing
                - At regular intervals feed the samples generated from the model

\

- Backpropagation through Time (BPTT)
    - Unrolling the computation graph along time axis
    - $h_t = W_{hx}x_t + W_{hh}h_{t-1} = f(x_t, h_{t-1}, w_h)$
    - $o_t = W_{ho}h_t = g(h_t, w_{oh})$
    - $L = {1 \over T}\sum l(y_t, o_t)$
    - ${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta w_h}$
    - ${\delta L \over \delta w_h} = {1 \over T} \sum {\delta l \over \delta o_t} {\delta o_t \over \delta h_t} {\delta h_t \over \delta w_h}$
    - ${\delta h_t \over \delta w_h} = {\delta h_t \over \delta w_h} + {\delta h_t \over \delta h_{t-1}} {\delta h_{t-1} \over \delta w_h}$
    - Common to truncate the update to length of the longest subsequence in the batch
    - As the sequence goes forward, the hidden state keeps getting multiplied by W(hh)
    - Gradients can decay or explode as we go backwards in time
    - Solution is to use additive rather than multiplicative updates

\

- Gated Recurrent Units
    - Learn when to update the hidden state by using a gating unit
    - Update Gate: Selectively remember important pieces of information
        - $Z_t = \sigma(W_{xz} X_t + W_{hz} H_{t-1})$
    - Reset Gate: Forget things and reset the hidden state when information is no longer useful
        - $R_t = \sigma(W_{rx} X_t + W_{rh} H_{t-1})$
    - Candidate State
        - Combine old memories that are not reset
        - $\tilde H_t = \tanh ( W_{xh} X_t + W_{hh} R_t \times H_{t-1})$
        - If reset is close to 1, standard RNN
        - If reset close to 0, standard MLP
        - Captures new short term information
    - New State
        - $H_t = Z_t H_{t-1} + (1 - Z_t) \tilde H_t$
        - Captures long term dependecies
        - If Z is close to 1, the hidden state carries as is and new inputs are ignored

\

- Long Short Term Memory (LSTM)
    - More sophisticated version of GRU
    - Augment the hidden state with memory cell
    - Three gates control this cell
        - Input: $I_t = \sigma( W_{ix} X_t + W_{ih} H_{t-1})$, what gets read in
        - Output: $O_t = \sigma(W_{ox} X_t + W_{oh} H_{t-1})$, what gets read out
        - Forget: $F_t = \sigma (W_{fx} X_t + W_{fh} H_{t-1})$, when the cell is reset
    - Candidate Cell State
        - $\tilde C_t = \tanh ( W_{cx} X_t + W_{ch} H_{t-1})$
    - Actual Candidate:
        - $C_t = F_{t} \times C_{t-1} + I_t  \times  \tilde C_{t}$
    - Hidden State
        - $H_t = O_t \times \tanh(C_t)$
        - Both output and hidden state for next time step
        - Hence, captures short term memory
    - The memory cell state captures long term memory
    - Peephole Connections
        - Pass cell state as additional input to the gates
    - *How does LSTM solve vanishing gradients problem?*

\

- Decoding
    - Output is generated one token at a time
    - Simple Solution: Greedy Decoding
        - Argmax over vocab at each step
        - Keep sampling unless <EOS> token output
    - May not be globally optimal path
    - Alternative: Beam Search
        - Compute top-K candidate outputs at each step
        - Expand each one in V possible ways
        - Total VK candidates generated 
    - GPT used top-k and top-p sampling
        - Top-K sampling: Redistribute the probability mass
        - Top-P sampling: Sample till the cumulative probability exceeds p

\

- Attention
    - In RNNs, hidden state linearly combines the inputs and then sends them to an activation function
    - Attention mechanism allows for more flexibility.
        - Suppose there are m feature vectors or values
        - Model decides which to use based on the input query vector q and its similarity to a set of m keys
        - If query is most similar to key i, then we use value i.
    - Attention acts as a soft dictionary lookup
        - Compare query q to each key k(i)
        - Retrieve the corresponding value v(i)
        - To make the operation differentiable:
            - Compute a convex combination
        - $Attn(q,(k_1,v_1),(k_2, v_2)...,(k_m,v_m)) = \sum_{i=1}^m \alpha_i (q, \{k_i\}) v_i$
            - $\alpha_i (q, \{k_i\})$ are the attention weights
        - Attention weights are computed from an attention score function $a(q,k_i)$
            - Computes the similarity between query and key
        - Once the scores are computed, use soft max to impose distribution
        - Masking helps in ignoring the index which are invalid while computing soft max
        - For computational efficiency, set the dim of query and key to be same (say d)
            - The similarity is given by dot product
            - The weights are randomly initialized
            - The expected variance of dot product will be d.
            - Scale the dot product by $\sqrt d$
            - Scaled Dot-Product Attention
                - Attention Weight: $a(q,k) = {q^Tk \over \sqrt d}$
                - Scaled Dot Product Attention: $Attn(Q,K,V) =  S({QK^T \over \sqrt d})V$
        - Example: Seq2Seq with Attention
            - Consider encoder-decoder architecture
            - In the decoder:
                - $h_t = f(h_{t-1}, c)$
                - c is the context vector from encoder
                - Usually the last hidden state of the encoder
            - Attention allows the decoder to look at all the input words
                - Better alignment between source and target
            - Make the context dynamic
                - Query: previous hidden state of the decoder
                - Key: all the hidden states from the encoder
                - Value: all the hidden states from the encoder
                - $c_t = \sum_{i=1}^T \alpha_i(h_{t-1}^d, \{h_i^e\})h_i^e$
            - If RNN has multiple hidden layers, usually take the top most layer
            - Can be extended to Seq2Vec models

\

- Transformers
    - Transformers are seq2seq models using attention in both encoder and decoder steps
    - Eliminate the need for RNNs
    - Self Attention:
        - Modify the encoder such that it attends to itself
        - Given a sequence of input tokens $[x_1, x_2, x_3...,x_n]$
        - Sequence of output tokens: $y_i = Attn(x_i, (x_1,x_1), (x_2, x_2)...,(x_n, x_n))$
            - Query is xi
            - Keys and Values are are x1,x2…xn (all valid inputs)
        - In the decoder step:
            - $y_i = Attn(y_{i-1}, (y_1,y_1), (y_2, y_2)...(y_{i-1}, y_{i-1}))$
            - Each new token generated has access to all the previous output
    - Multi-Head Attention
        - Use multiple attention matrices to capture different nuances and similarities
        - $h_i = Attn(W_i^q q_i, (W_i^k k_i, W_i^v v_i))$
        - Stack all the heads together and use a projection matrix to get he output
        - Set $p_q h = p_k h = p_v h = p_o$ for parallel computation **How?
    - Positional Encoding
        - Attention is permutation invariant
        - Positional encodings help overcome this
        - Sinusoidal Basis
        - Positional Embeddings are combined with original input X → X + P
    - Combining All the Blocks
        - Encoder
            - Input: $ Z = LN(MHA(X,X,X) + X$
            - Encoder: $E = LN(FF(Z) + Z)$
                - For the first layer:
                    - $ Z = \text{POS}(\text{Embed}(X))$
        - In general, model has N copies of the encoder
        - Decoder 
            - Has access to both: encoder and previous tokens
            - Input: $ Z = LN(MHA(X,X,X) + X$
            - Input $ Z = LN(MHA(Z,E,E) + Z$

\

- Representation Learning
    - Contextual Word Embeddings
        - Hidden state depends on all previous tokens
        - Use the latent representation for classification / other downstream tasks
        - Pre-train on a large corpus 
        - Fine-tune on small task specific dataset
        - Transfer Learning
    - ELMo
        - Embeddings from Language Model
        - Fit two RNN models
            - Left to Right
            - Right to Left
        - Combine the hidden state representations to fetch embedding for each word
    - BERT
        - Bi-Directional Encoder Representations from Transformers
        - Pre-trained using Cloze task (MLM i.e. Masked Language Modeling)
        - Additional Objective: Next sentence Prediction
    - GPT 
        - Generative Pre-training Transformer
        - Causal model using Masked Decoder
        - Train it as a language model on web text
    - T5
        - Text-to-Text Transfer Transformer
        - Single model to perform multiple tasks
        - Tell the task to perform as part of input sequence

